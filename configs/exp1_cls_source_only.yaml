exp_name: exp1_cls_source_only  # Experiment name

# Logger options
resume: False                   # Wether resume training states
image_save_iter:                # How often do you want to save output images during training
image_display_iter:             # How often do you want to display output images during training
snapshot_save_iter: 1500        # How often do you want to save trained models
log_iter: 100                   # How often do you want to log the training stats
eval_iter: 1500                 # How often do you want to evaluate the model in validation set

# Optimization options
max_epoch: 50                  # Maximum number of training epochs
max_iter: 1000000               # Maximum number of training iterations
cls_batch_size: 32              # Batch size in training
seg_batch_size:                 # Batch size in training
test_size: 16                   # Batch size in testing
momentum: 0.9
weight_decay: 0.0005            # Weight decay
lr: 0.0001                      # Initial learning rate
lr_policy: constant             # Learning rate scheduler [step/multi_step/constant]
step_size:                      # How often to decay learning rate
gamma:                          # How much to decay learning rate
init: kaiming                   # Initialization [gaussian/kaiming/xavier/orthogonal]

# Loss function weights
cls_w: 1                        # Weight of classifier loss

# Model options
task:       Classification      # [Classification/Segmentation/Multi_task]
trainer:    Classify
translator:                     # [None/Latent]
classifier: ResNet101           # [None/ResNet50/ResNet101]
segmentor:                      # [None/FCN8sRes50]
freeze_list: []                 # ['gen_a_enc', 'segmentor', 'classifier', 'gen_b_enc', 'dis_h', 'enc_bridge']

# Using pretrained model
pretrained_checkpoint:              # Directory for pretrained model.

# Data options
num_class: {'cls': 12, 'seg': 20}   # Number of categories
num_workers: 4                      # Number of data loading threads.
new_size:                           # First resize the shortest image side to this size
    {'classification': 256,
     'segmentation':   }
crop:                               # [Wether crop the images, crop height, crop width, wether pad after crop]
    {'classification': [True, 224, 224],
     'segmentation':   }

# Path and directory
data_root_cls:                      # Data root. Using for prepare dataset and data loader
    "./datasets/VisDA_17/closedset_classification"
data_root_seg:
    "./datasets/VisDA_17/semantic_segmentation"
data_list_s:
    {'train': 'source_train_list.txt', 'val': 'source_val_list.txt', 'test': 'source_test_list.txt'}
data_list_t:
    {'train': 'target_train_list.txt', 'val': 'target_val_list.txt', 'test': 'target_test_list.txt'}

category:                           # Dataset category
    ['aeroplane', 'bicycle', 'bus','car',
     'horse', 'knife', 'motorcycle', 'person',
     'plant', 'skateboard', 'train', 'truck']
